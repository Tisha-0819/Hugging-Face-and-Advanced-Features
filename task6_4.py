# -*- coding: utf-8 -*-
"""Task6_4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ad3sQL9rBiHu_pNDVlLkVIhAcedZVMZB
"""

!pip install -q transformers datasets accelerate

from google.colab import userdata
os.environ["HF_TOKEN"] = userdata.get('Hftoken')

import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    provider="hf-inference",
    api_key=os.environ["HF_TOKEN"],
)

result = client.fill_mask(
    "The answer to the universe is [MASK].",
    model="distilbert/distilbert-base-uncased",
)

from datasets import Dataset

data = {
    "text": [
        "I love this movie",
        "This film was terrible",
        "Amazing acting and story",
        "Worst movie ever",
        "I enjoyed the film",
        "Not good, very boring"
    ],
    "label": [1, 0, 1, 0, 1, 0]   # 1 = positive, 0 = negative
}

dataset = Dataset.from_dict(data)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

def tokenize_data(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True
    )

tokenized_dataset = dataset.map(tokenize_data)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=2,
    logging_steps=1,
    save_strategy="no",
    report_to="none"
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer
)

trainer.train()

from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer
)

classifier("I really liked this movie")